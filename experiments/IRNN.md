# A Simple Way to Initialize Recurrent Networks of Rectified Linear Units

http://arxiv.org/abs/1504.00941

# Key idea

Initializing recurrent weight matrix in RNN with identity matrix, and using ReLU for activation

# Experiment notes

The empirical results are able to reproduce but not easy. The training process is highly unstable.


